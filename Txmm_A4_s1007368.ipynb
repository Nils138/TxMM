{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e464e4",
   "metadata": {},
   "source": [
    "# A4: Authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87120e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "from nltk import ngrams, pos_tag\n",
    "import nltk\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from string import punctuation, ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b3447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nils\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download the 'stopwords' and 'punkt' from the Natural Language Toolkit, you can comment the next lines if already present.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3115677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into memory from the filesystem\n",
    "def load_data(dir_name):\n",
    "    return sklearn.datasets.load_files('data/%s' % dir_name, encoding='utf-8')\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    return load_data('train')\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    return load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c3649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify using the features\n",
    "def classify(train_features, train_labels, test_features):\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(train_features, train_labels)\n",
    "    return clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6abe202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions (y_pred) given the ground truth (y_true)\n",
    "def evaluate(y_true, y_pred):\n",
    "    # TODO: What is being evaluated here and what does it say about the performance? Include or change the evaluation\n",
    "    # TODO: if necessary.\n",
    "    # A: y_true is the actual author, while y_pred is the predicted author by the program. These values are fairly \n",
    "    # representative about the quality of a classifier and the features.\n",
    "    recall = sklearn.metrics.recall_score(y_true, y_pred, average='macro')\n",
    "    print(\"Recall: %f\" % recall)\n",
    "\n",
    "    precision = sklearn.metrics.precision_score(y_true, y_pred, average='macro')\n",
    "    print(\"Precision: %f\" % precision)\n",
    "\n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred, average='macro')\n",
    "    print(\"F1-score: %f\" % f1_score)\n",
    "\n",
    "    return recall, precision, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fdf100b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a given text\n",
    "def extract_features(text):\n",
    "    bag_of_words = [x for x in wordpunct_tokenize(text)]\n",
    "    sentences = [x for x in sent_tokenize(text)]\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Example feature 1: count the number of words\n",
    "    features.append(len(bag_of_words))\n",
    "    \n",
    "    # Example feature 2: count the number of words, excluded the stopwords\n",
    "    no_stopwords = [x for x in bag_of_words if x.lower() not in stop_words]\n",
    "    features.append(len(no_stopwords))\n",
    "\n",
    "    # Feature 2-4: take top k most frequent word counts\n",
    "    k = 3\n",
    "    freq_dict = {i:no_stopwords.count(i) for i in set(no_stopwords)}\n",
    "    features.extend(sorted(freq_dict.values(), reverse=True)[:k])\n",
    "    n = k - len(freq_dict)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "        n -= 1\n",
    "    \n",
    "    # Feature 5: number of stopwords\n",
    "    features.append(len([x for x in bag_of_words if x.lower() in stop_words]))\n",
    "    \n",
    "    # Feature 6: average sentence length\n",
    "    sentence_lengths = [len(x) for x in sentences]\n",
    "    features.append(sum(sentence_lengths) / len(sentence_lengths))\n",
    "    \n",
    "    # Feature 7-24: top m most frequent characters\n",
    "    m = 17\n",
    "    chars = list(text)\n",
    "    char_dict = {i:chars.count(i) for i in set(chars)}\n",
    "    #features.extend(sorted(char_dict.values(), reverse=True)[:m])\n",
    "    #n = m - len(char_dict)\n",
    "    #while n > 0:\n",
    "    #    features.append(0)\n",
    "    #    n -= 1\n",
    "        \n",
    "    # Total character length\n",
    "    features.append(len(chars))\n",
    "    \n",
    "    # Feature 25-38: most frequent n-grams\n",
    "    n = 4\n",
    "    L = 13\n",
    "    n_grams = []\n",
    "    for gram in ngrams(bag_of_words, n):  # I hate this, why doesn't ngrams() return a list but a generator \n",
    "        n_grams.append(gram)\n",
    "    ngram_dict = sorted({i:n_grams.count(i) for i in set(n_grams)}.values(), reverse=True)\n",
    "    features.extend(ngram_dict[:L])\n",
    "    n = L - len(ngram_dict)\n",
    "    while n > 0:\n",
    "        print(\"L\")\n",
    "        features.append(0)\n",
    "        n -= 1\n",
    "    \n",
    "    # Feature 195-204: N-grams 2: Electric Boogaloo\n",
    "    n = 2\n",
    "    L = 9\n",
    "    n_grams = []\n",
    "    for gram in ngrams(bag_of_words, n):  # I hate this, why doesn't ngrams() return a list but a generator \n",
    "        n_grams.append(gram)\n",
    "    features.extend(sorted({i:n_grams.count(i) for i in set(n_grams)}.values(), reverse=True)[:L])\n",
    "    n = L - len(n_grams)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "    \n",
    "    # Feature 205-207: N-grams 3: Tokyo Drift\n",
    "    n = 3\n",
    "    L = 2\n",
    "    n_grams = []\n",
    "    for gram in ngrams(bag_of_words, n):  # I hate this, why doesn't ngrams() return a list but a generator \n",
    "        n_grams.append(gram)\n",
    "    features.extend(sorted({i:n_grams.count(i) for i in set(n_grams)}.values(), reverse=True)[:L])\n",
    "    n = L - len(n_grams)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "        \n",
    "     # Feature 40-130: Function words frequency\n",
    "    function_words=\"a between in nor some upon about both including nothing somebody us above but inside of someone used after by into off something via all can is on such we although cos it once than what am do its one that whatever among down latter onto the when an each less opposite their where and either like or them whether another enough little our these which any every lots outside they while anybody everybody many over this who anyone everyone me own those whoever anything everything more past though whom are few most per through whose around following much plenty till will as for must plus to with at from my regarding toward within be have near same towards without because he need several under worth before her neither she unless would behind him no should unlike yes below i nobody since until you beside if none so up your\"\n",
    "    function_words = function_words.split(\" \")\n",
    "    p = 90\n",
    "    frequencies = []\n",
    "    for i in range(p):\n",
    "        key = function_words[i]\n",
    "        frequencies.append(freq_dict.get(function_words[i]) if key in freq_dict else 0)\n",
    "    features.extend(frequencies)\n",
    "    \n",
    "    # Feature 133-156: Pos tag frequencies\n",
    "    pos_tags = [y for x, y in nltk.pos_tag(bag_of_words)]\n",
    "    q = 23\n",
    "    pos_dict = sorted({i:pos_tags.count(i) for i in set(pos_tags)}.values(), reverse=True)\n",
    "    features.extend(pos_dict[:q])\n",
    "    n = q - len(pos_dict)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "        n -= 1\n",
    "        \n",
    "    # Feature 39: Punctuation\n",
    "    features.append(len(text.translate(str.maketrans('', '', punctuation))))\n",
    "    \n",
    "    # Feature 132: Question marks\n",
    "    features.append(char_dict.get('?') if '?' in char_dict else 0)\n",
    "    \n",
    "    # Feature 133: Semicolons\n",
    "    features.append(char_dict.get(';') if ';' in char_dict else 0)\n",
    "    \n",
    "    # Feature 133: Quotes\n",
    "    features.append(char_dict.get('\"') if '\"' in char_dict else 0)\n",
    "    features.append(char_dict.get(\"'\") if \"'\" in char_dict else 0)\n",
    "    \n",
    "    # Feature 133: Semicolons\n",
    "    features.append(char_dict.get('/') if '/' in char_dict else 0)\n",
    "    \n",
    "    # Feature 133: Semicolons\n",
    "    features.append(char_dict.get('(') if '(' in char_dict else 0)\n",
    "    features.append(char_dict.get(')') if ')' in char_dict else 0)\n",
    "    \n",
    "    # Mathematical symbols\n",
    "    features.append(char_dict.get('+') if '+' in char_dict else 0)\n",
    "    features.append(char_dict.get('-') if '-' in char_dict else 0)\n",
    "    features.append(char_dict.get('*') if '*' in char_dict else 0)\n",
    "    features.append(char_dict.get('=') if '=' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Periods\n",
    "    features.append(char_dict.get('.') if '.' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Tabs\n",
    "    features.append(char_dict.get('\\t') if '\\t' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Whitespaces\n",
    "    features.append(char_dict.get('\\n') if '\\n' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Commas\n",
    "    features.append(char_dict.get(',') if ',' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Colons\n",
    "    features.append(char_dict.get(':') if ':' in char_dict else 0)\n",
    "    \n",
    "    # Feature 184: Spaces\n",
    "    features.append(char_dict.get(' ') if ' ' in char_dict else 0)\n",
    "    \n",
    "    # Numbers\n",
    "    digits = {i:chars.count(i) for i in set(chars) if i.isdigit()}\n",
    "    for c in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "        if not c in digits:\n",
    "            digits[c] = 0\n",
    "    features.extend(digits.values())\n",
    "        \n",
    "    # Feature 157-183: Capital letter counts\n",
    "    cap_raw = {i:chars.count(i) for i in set(chars) if i.isalpha() and i.isupper()}\n",
    "    capitals = {}\n",
    "    for c in ascii_uppercase:\n",
    "        if not c in cap_raw:\n",
    "            capitals[c] = 0\n",
    "        else:\n",
    "            capitals[c] = cap_raw[c]\n",
    "    features.extend(capitals.values())\n",
    "    \n",
    "    # Pos-Tags vectorized\n",
    "    q = 25\n",
    "    pos_dict = {x:y for x, y in nltk.pos_tag(bag_of_words)}        \n",
    "    vec = DictVectorizer()\n",
    "    ar = vec.fit_transform(pos_dict).toarray()[0]\n",
    "    features.extend(ar[:q])\n",
    "    n = q - len(ar)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "        n -= 1\n",
    "    \n",
    "    q = 25\n",
    "    pos_bag = nltk.pos_tag(bag_of_words)\n",
    "    pos_dict = {y:pos_bag.count(y) for x, y in pos_bag}        \n",
    "    vec = DictVectorizer()\n",
    "    ar = vec.fit_transform(pos_dict).toarray()[0]\n",
    "    features.extend(ar[:q])\n",
    "    n = q - len(ar)\n",
    "    while n > 0:\n",
    "        features.append(0)\n",
    "        n -= 1\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bcd5ffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Recall: 0.544444\n",
      "Precision: 0.505556\n",
      "F1-score: 0.508148\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.522222\n",
      "Precision: 0.481481\n",
      "F1-score: 0.479259\n",
      "\n",
      "Fold 3\n",
      "Recall: 0.533333\n",
      "Precision: 0.466667\n",
      "F1-score: 0.479259\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.611111\n",
      "Precision: 0.481481\n",
      "F1-score: 0.514815\n",
      "\n",
      "Fold 5\n",
      "Recall: 0.622222\n",
      "Precision: 0.520370\n",
      "F1-score: 0.542963\n",
      "\n",
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.566667\n",
      "Precision: 0.488889\n",
      "F1-score: 0.496296\n",
      "\n",
      "Fold 7\n",
      "Recall: 0.555556\n",
      "Precision: 0.491481\n",
      "F1-score: 0.492593\n",
      "\n",
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.555556\n",
      "Precision: 0.483333\n",
      "F1-score: 0.499259\n",
      "\n",
      "Fold 9\n",
      "Recall: 0.555556\n",
      "Precision: 0.479630\n",
      "F1-score: 0.493333\n",
      "\n",
      "Fold 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.577778\n",
      "Precision: 0.507407\n",
      "F1-score: 0.522963\n",
      "\n",
      "Averaged total recall 0.5644444444444444\n",
      "Averaged total precision 0.49062962962962964\n",
      "Averaged total f-score 0.5028888888888889\n",
      "Recall: 0.578519\n",
      "Precision: 0.611720\n",
      "F1-score: 0.565844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\TxMM\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5785185185185185, 0.6117195767195768, 0.5658441558441559)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_train_data()\n",
    "\n",
    "# Extract the features\n",
    "features = list(map(extract_features, train_data.data))\n",
    "\n",
    "# Classify and evaluate\n",
    "skf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "scores = []\n",
    "for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):\n",
    "    # Print the fold number\n",
    "    print(\"Fold %d\" % (fold_id + 1))\n",
    "\n",
    "    # Collect the data for this train/validation split\n",
    "    train_features = [features[x] for x in train_indexes]\n",
    "    train_labels = [train_data.target[x] for x in train_indexes]\n",
    "    validation_features = [features[x] for x in validation_indexes]\n",
    "    validation_labels = [train_data.target[x] for x in validation_indexes]\n",
    "\n",
    "    # Classify and add the scores to be able to average later\n",
    "    y_pred = classify(train_features, train_labels, validation_features)\n",
    "    scores.append(evaluate(validation_labels, y_pred))\n",
    "\n",
    "    # Print a newline\n",
    "    print(\"\")\n",
    "\n",
    "# Print the averaged score\n",
    "recall = sum([x[0] for x in scores]) / len(scores)\n",
    "print(\"Averaged total recall\", recall)\n",
    "precision = sum([x[1] for x in scores]) / len(scores)\n",
    "print(\"Averaged total precision\", precision)\n",
    "f_score = sum([x[2] for x in scores]) / len(scores)\n",
    "print(\"Averaged total f-score\", f_score)\n",
    "\n",
    "# TODO: Once you are done crafting your features and tuning your model, also test on the test set and report your\n",
    "# TODO: findings. How does the score differ from the validation score? And why do you think this is?\n",
    "test_data = load_test_data()\n",
    "test_features = list(map(extract_features, test_data.data))\n",
    "y_pred = classify(features, train_data.target, test_features)\n",
    "evaluate(test_data.target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0992da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : None\n",
      "2 : Punctuation\n",
      "3 : Digits\n",
      "4 : Capitals\n",
      "5 : POS-tags\n",
      "6 : Function word frequency\n",
      "7 : N-gram frequency\n",
      "8 : Character frequency\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "Fscores = [0.53, 0.47, 0.52, 0.41, 0.44, 0.45, 0.45, 0.50]\n",
    "features = ['None', 'Punctuation', 'Digits', 'Capitals', 'POS-tags', 'Function word frequency', 'N-gram frequency', 'Character frequency']\n",
    "for i, f in enumerate(features):\n",
    "    print(i + 1, \":\", f)\n",
    "plt.title(\"Ablation analysis\")\n",
    "plt.bar(np.arange(1, len(features) + 1), Fscores)\n",
    "plt.xlabel(\"Removed feature\")\n",
    "plt.ylabel(\"F-score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d003abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
